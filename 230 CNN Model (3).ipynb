{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib \n",
    "import pandas as pd\n",
    "from tensorflow.python.framework import ops\n",
    "import urllib\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "import scipy\n",
    "import IPython\n",
    "import sys\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 1} ) \n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "with tf.device('/gpu:0'):\n",
    "    def format_data(filename):\n",
    "        training = open(filename)\n",
    "        header = training.readline()\n",
    "        fields = header.strip().replace('\"','').split(',')\n",
    "        featureNames = fields[:-1]\n",
    "        labelName = fields[-1]\n",
    "        X = []\n",
    "        Y = []\n",
    "        prev_one = [float(x) for x in training.readline().split(',')]\n",
    "        prev_two = [float(x) for x in training.readline().split(',')]\n",
    "        prev_three = [float(x) for x in training.readline().split(',')]\n",
    "        prev_four = [float(x) for x in training.readline().split(',')]\n",
    "        prev_five = [float(x) for x in training.readline().split(',')]\n",
    "        y_line = [float(x) for x in training.readline().split(',')]\n",
    "        for l in training:\n",
    "            prev_flights = np.vstack((prev_one,prev_two,prev_three,prev_four,prev_five))\n",
    "            X.append(prev_flights)\n",
    "            Y.append(y_line[-1])\n",
    "            prev_one = prev_two\n",
    "            prev_two = prev_three\n",
    "            prev_three = prev_four\n",
    "            prev_four = prev_five\n",
    "            prev_five = y_line\n",
    "            y_line = [float(x) for x in l.split(',')]\n",
    "            \n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        Y = Y.astype(int)\n",
    "        nb_classes = 15\n",
    "        Y = np.eye(nb_classes)[Y]\n",
    "        X = X\n",
    "        Y = np.roll(Y,2)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Format Training Set\n",
    "X_train, Y_train = format_data('training.csv')\n",
    "X_train = X_train.reshape((X_train.shape[0], 5, 13, 1))\n",
    "\n",
    "# Import and Format Dev Set\n",
    "X_dev, Y_dev = format_data('dev.csv')\n",
    "X_dev = X_dev.reshape((X_dev.shape[0], 5, 13, 1))\n",
    "\n",
    "num_flights = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Placeholder needed for latter use\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_H0, n_W0, n_C0), name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y), name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary parameters for CNN\n",
    "def initialize_parameters():\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [num_flights, num_flights, 1, 16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [num_flights, num_flights, 16, 32], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [num_flights, num_flights, 32, 64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [num_flights, num_flights, 64, 128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [num_flights, num_flights, 128, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"W4\": W4,\n",
    "                  \"W5\": W5}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward_propagation Function\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "    \n",
    "\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "    A1 =  tf.nn.relu(Z1)\n",
    "\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,num_flights,num_flights,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "    Z2 = tf.nn.conv2d(A1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,num_flights,num_flights,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    Z3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    P3 = tf.nn.max_pool(A3, ksize = [1,num_flights,num_flights,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    Z4 = tf.nn.conv2d(P3,W4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    \n",
    "    P4 = tf.nn.max_pool(A4, ksize = [1,num_flights,num_flights,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    Z5 = tf.nn.conv2d(P4,W5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    \n",
    "    # FLATTEN\n",
    "    P5 = tf.contrib.layers.flatten(A5)\n",
    "    \n",
    "    # FULLY-CONNECTED without non-linear activation function\n",
    "\n",
    "    Z6 = tf.contrib.layers.fully_connected(P5, num_outputs = 15, activation_fn=None)\n",
    "\n",
    "    return Z6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_cost using softmax cross entropy\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the false and negative positive/negative existed in the prediction\n",
    "# Used for latter calculation of evaluation metrics\n",
    "\n",
    "def model_eval(predict_op, truth_label):\n",
    "    counter_correct, counter_tp, counter_fn, counter_fp, counter_tn = np.zeros(5)\n",
    "    total_prediction = truth_label.shape[0]\n",
    "    for group in range(truth_label.shape[0]):\n",
    "        if (truth_label[group] > 3 and predict_op[group] > 3):\n",
    "            counter_tn += 1\n",
    "        if (truth_label[group] <= 3 and predict_op[group] <= 3):\n",
    "            counter_tp += 1\n",
    "        if (truth_label[group] > 3 and predict_op[group] <= 3):\n",
    "            counter_fp += 1\n",
    "        if (truth_label[group] <= 3 and predict_op[group] > 3):\n",
    "            counter_fn += 1\n",
    "        if (truth_label[group] == predict_op[group]):\n",
    "            counter_correct += 1\n",
    "    \n",
    "    return counter_correct, counter_tp, counter_fn, counter_fp, counter_tn \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics of the model\n",
    "def model_metrics(counter_tp, counter_fn, counter_fp, counter_tn):\n",
    "    binary_accuracy = (counter_tp + counter_tn)/(counter_tp + counter_tn + counter_fp + counter_fn)\n",
    "    binary_precision = (counter_tp)/(counter_tp + counter_fp)\n",
    "    binary_recall = (counter_tp)/(counter_tp + counter_fn)\n",
    "    f_1 = (2*binary_precision*binary_recall)/(binary_precision + binary_recall)\n",
    "    \n",
    "    return binary_accuracy, f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN Model\n",
    "\n",
    "def model(X_train, Y_train, X_dev, Y_dev, learning_rate = 0.001,\n",
    "          num_epochs = 60, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape \n",
    "    n_y = Y_train.shape[1]  \n",
    "    training_costs = []                                          # To keep track of the cost\n",
    "    dev_costs = []\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Create Mini-Batches\n",
    "    X_minibatches = np.array_split(X_train, int(X_train.shape[0]/128))\n",
    "    Y_minibatches = np.array_split(Y_train, int(Y_train.shape[0]/128))\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            for minibatch in range(len(Y_minibatches)):\n",
    "                _ , training_cost = sess.run([optimizer, cost], feed_dict={X: X_minibatches[minibatch], Y: Y_minibatches[minibatch]}) \n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                dev_cost = sess.run(cost, feed_dict={X: X_dev, Y: Y_dev})\n",
    "                training_costs.append(training_cost)\n",
    "                dev_costs.append(dev_cost)\n",
    "                print (\"Training Cost after epoch %i: %f\" % (epoch, training_cost))\n",
    "                print (\"Dev Cost after epoch %i: %f\" % (epoch, dev_cost))\n",
    "                 \n",
    "        # Plot the cost\n",
    "        plt.plot(np.squeeze(training_costs))\n",
    "        plt.plot(np.squeeze(dev_costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (epochs)')\n",
    "        plt.title(\"CNN model loss\")\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on Training set and print evaluation metrics\n",
    "        counter_correct, counter_tp, counter_fn, counter_fp, counter_tn = np.zeros(5)\n",
    "        for minibatch in range(len(Y_minibatches)):\n",
    "            predict = tf.argmax(Z, 1)\n",
    "            predict_op = predict.eval({X: X_minibatches[minibatch], Y: Y_minibatches[minibatch]})\n",
    "            truth_label = np.argmax(Y_minibatches[minibatch], 1)\n",
    "            counter_correct_m, counter_tp_m, counter_fn_m, counter_fp_m, counter_tn_m = model_eval(predict_op, truth_label)\n",
    "            counter_tp += counter_tp_m\n",
    "            counter_fn += counter_fn_m\n",
    "            counter_fp += counter_fp_m\n",
    "            counter_tn += counter_tn_m\n",
    "            counter_correct += counter_correct_m\n",
    "        train_binary_accuracy, train_f_1 = model_metrics(counter_tp, counter_fn, counter_fp, counter_tn)\n",
    "        train_accuracy = counter_correct/X_train.shape[0]\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Train Accuracy (binary): \", train_binary_accuracy)\n",
    "        print(\"Train F-1 (binary): \", train_f_1)\n",
    "\n",
    "        # Evaluate on Dev set and print evaluation metrics\n",
    "        predict = tf.argmax(Z, 1)\n",
    "        predict_op = predict.eval({X: X_dev, Y: Y_dev})\n",
    "        truth_label = np.argmax(Y_dev, 1)\n",
    "        counter_correct, counter_tp, counter_fn, counter_fp, counter_tn = model_eval(predict_op, truth_label)\n",
    "        dev_binary_accuracy, dev_f_1 = model_metrics(counter_tp, counter_fn, counter_fp, counter_tn)\n",
    "        dev_accuracy = counter_correct/X_dev.shape[0]\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "        print(\"Dev Accuracy (binary): \", dev_binary_accuracy)\n",
    "        print(\"Dev F-1 (binary): \", dev_f_1)\n",
    "            \n",
    "        return train_accuracy, dev_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_, parameters = model(X_train, Y_train, X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
